# ğŸ—£ï¸ Natural Language Processing (NLP)

Ce dossier contient les projets liÃ©s au traitement du langage naturel (NLP) avec TensorFlow et Keras.

---

## ğŸ“‚ Contenu

### ğŸ“˜ sarcasm.ipynb - DÃ©tection de Sarcasme avec RNNs

**Objectif** : DÃ©velopper un modÃ¨le de Deep Learning capable de dÃ©tecter le sarcasme dans des textes en utilisant des rÃ©seaux de neurones rÃ©currents.

**Concepts couverts** :

#### 1ï¸âƒ£ PrÃ©traitement du texte
- âœ… **Tokenization** : conversion de phrases en sÃ©quences numÃ©riques
  - Utilisation de `Tokenizer` de Keras
  - CrÃ©ation d'un vocabulaire (word_index)
- âœ… **Padding** : uniformisation de la longueur des sÃ©quences
  - `pad_sequences` avec diffÃ©rentes stratÃ©gies (pre/post)
- âœ… **TextVectorization** : layer Keras pour vectorisation
- âœ… Gestion du vocabulaire et des mots hors vocabulaire (OOV)

#### 2ï¸âƒ£ Embeddings
- âœ… **Word Embeddings** : reprÃ©sentation dense des mots
  - Embedding layer dans Keras
  - Dimensions d'embedding
  - Embeddings appris vs prÃ©-entraÃ®nÃ©s
- âœ… Visualisation des embeddings

#### 3ï¸âƒ£ Architectures de Deep Learning pour NLP

**ModÃ¨les implÃ©mentÃ©s** :
- ğŸ”¹ **RNN simple** (Recurrent Neural Network)
  - Architecture basique pour sÃ©quences
  - ProblÃ¨mes de gradient vanishing
  
- ğŸ”¹ **LSTM** (Long Short-Term Memory)
  - Meilleure capture des dÃ©pendances Ã  long terme
  - Gates : forget, input, output
  
- ğŸ”¹ **GRU** (Gated Recurrent Unit)
  - Version simplifiÃ©e du LSTM
  - Moins de paramÃ¨tres
  
- ğŸ”¹ **Bidirectional RNN/LSTM**
  - Traitement dans les deux sens (forward + backward)
  - Meilleure comprÃ©hension du contexte

#### 4ï¸âƒ£ EntraÃ®nement et Ã©valuation
- âœ… Compilation des modÃ¨les (optimizer, loss, metrics)
- âœ… EntraÃ®nement avec callbacks
- âœ… Visualisation des courbes d'apprentissage
- âœ… MÃ©triques de classification :
  - Accuracy
  - Precision, Recall, F1-score
  - Matrice de confusion
- âœ… PrÃ©dictions sur textes individuels
- âœ… Batch predictions

#### 5ï¸âƒ£ Techniques avancÃ©es
- âœ… **Dropout** pour rÃ©gularisation
- âœ… **Early Stopping** pour Ã©viter le surapprentissage
- âœ… **Learning Rate Scheduling**
- âœ… Comparaison de diffÃ©rentes architectures

---

## ğŸ¯ Dataset

**Sarcasm Detection Dataset** : phrases Ã©tiquetÃ©es comme sarcastiques ou non-sarcastiques

Structure typique :
- **Texte** : phrases/commentaires
- **Label** : 0 (non-sarcastique) ou 1 (sarcastique)
- **Contexte** : Ã©ventuellement des mÃ©tadonnÃ©es

---

## ğŸ› ï¸ Technologies utilisÃ©es

- **Python 3.x**
- **TensorFlow / Keras** : construction et entraÃ®nement des modÃ¨les
- **NumPy** : manipulation de donnÃ©es
- **Pandas** : chargement et exploration du dataset
- **Matplotlib / Seaborn** : visualisations
- **Scikit-learn** : mÃ©triques et preprocessing

---

## ğŸ“Š CompÃ©tences dÃ©veloppÃ©es

âœ”ï¸ PrÃ©traitement de texte : tokenization, padding, embeddings  
âœ”ï¸ Word embeddings et reprÃ©sentations vectorielles  
âœ”ï¸ RÃ©seaux de neurones rÃ©currents (RNN, LSTM, GRU)  
âœ”ï¸ Architecture bidirectionnelle  
âœ”ï¸ Classification de texte  
âœ”ï¸ Gestion de sÃ©quences de longueur variable  
âœ”ï¸ RÃ©gularisation et prÃ©vention du surapprentissage  
âœ”ï¸ Ã‰valuation de modÃ¨les NLP  

---

## ğŸš€ Comment utiliser

1. Charger le dataset de dÃ©tection de sarcasme
2. ExÃ©cuter le preprocessing du texte
3. EntraÃ®ner diffÃ©rents modÃ¨les (RNN, LSTM, GRU)
4. Comparer les performances
5. Faire des prÃ©dictions sur de nouveaux textes

---

## ğŸ’¡ Cas d'usage

- Analyse de sentiment
- DÃ©tection d'ironie/sarcasme sur les rÃ©seaux sociaux
- ModÃ©ration de contenu
- Chatbots et assistants conversationnels
- Analyse d'avis clients

---

## ğŸ“ Notes

Ce projet illustre l'application du Deep Learning au traitement du langage naturel, en particulier pour des tÃ¢ches de classification de texte nÃ©cessitant la comprÃ©hension du contexte et des subtilitÃ©s linguistiques comme le sarcasme.
